{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Preprocessing Different File Types\n",
    "\n",
    "- `FileTypeRouter`: ThÃ nh pháº§n nÃ y sáº½ giÃºp báº¡n Ä‘á»‹nh tuyáº¿n cÃ¡c tá»‡p dá»±a trÃªn loáº¡i MIME tÆ°Æ¡ng á»©ng cá»§a chÃºng Ä‘áº¿n cÃ¡c thÃ nh pháº§n khÃ¡c nhau\n",
    "- `MarkdownToDocument`: ThÃ nh pháº§n nÃ y sáº½ giÃºp báº¡n chuyá»ƒn Ä‘á»•i cÃ¡c file markdown thÃ nh Haystack Documents\n",
    "- `PyPDFToDocument`: ThÃ nh pháº§n nÃ y sáº½ giÃºp báº¡n chuyá»ƒn Ä‘á»•i file pdf thÃ nh Haystack Documents\n",
    "- `TextFileToDocument`: ThÃ nh pháº§n nÃ y sáº½ giÃºp báº¡n chuyá»ƒn Ä‘á»•i file vÄƒn báº£n thÃ nh Haystack Documents\n",
    "- `DocumentJoiner`: ThÃ nh pháº§n nÃ y sáº½ giÃºp báº¡n ná»‘i cÃ¡c TÃ i liá»‡u Ä‘áº¿n tá»« cÃ¡c nhÃ¡nh khÃ¡c nhau cá»§a Ä‘Æ°á»ng á»‘ng\n",
    "- `DocumentCleaner`(tÃ¹y chá»n): ThÃ nh pháº§n nÃ y sáº½ giÃºp báº¡n lÃ m cho TÃ i liá»‡u dá»… Ä‘á»c hÆ¡n báº±ng cÃ¡ch xÃ³a cÃ¡c khoáº£ng tráº¯ng thá»«a, v.v.\n",
    "- `DocumentSplitter`: ThÃ nh pháº§n nÃ y sáº½ giÃºp báº¡n chia TÃ i liá»‡u cá»§a mÃ¬nh thÃ nh nhiá»u pháº§n\n",
    "- `SentenceTransformersDocumentEmbedder`: ThÃ nh pháº§n nÃ y sáº½ giÃºp báº¡n táº¡o cÃ¡c pháº§n nhÃºng cho TÃ i liá»‡u.\n",
    "- `DocumentWriter`: ThÃ nh pháº§n nÃ y sáº½ giÃºp báº¡n ghi Documents vÃ o DocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install haystack-ai\n",
    "# pip install \"sentence-transformers>=2.2.0\" \"huggingface_hub>=0.22.0\"\n",
    "# pip install markdown-it-py mdit_plain pypdf\n",
    "# pip install gdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recipe_files/vegan_flan_recipe.md',\n",
       " 'recipe_files/vegan_keto_eggplant_recipe.pdf',\n",
       " 'recipe_files/vegan_sunflower_hemp_cheese_recipe.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/1n9yqq5Gl_HWfND5bTlrCwAOycMDt5EMj\"\n",
    "output_dir = \"recipe_files\"\n",
    "\n",
    "gdown.download_folder(url, quiet=True, output=output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Pipeline to Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngohongthai/miniconda3/envs/nmt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.converters import MarkdownToDocument, PyPDFToDocument, TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "file_type_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\", \"text/markdown\"])\n",
    "text_file_converter = TextFileToDocument()\n",
    "markdown_converter = MarkdownToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "document_joiner = DocumentJoiner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_cleaner = DocumentCleaner()\n",
    "document_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "document_writer = DocumentWriter(document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x31c8ede40>\n",
       "ðŸš… Components\n",
       "  - file_type_router: FileTypeRouter\n",
       "  - text_file_converter: TextFileToDocument\n",
       "  - markdown_converter: MarkdownToDocument\n",
       "  - pypdf_converter: PyPDFToDocument\n",
       "  - document_joiner: DocumentJoiner\n",
       "  - document_cleaner: DocumentCleaner\n",
       "  - document_splitter: DocumentSplitter\n",
       "  - document_embedder: SentenceTransformersDocumentEmbedder\n",
       "  - document_writer: DocumentWriter\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - file_type_router.text/plain -> text_file_converter.sources (List[Path])\n",
       "  - file_type_router.application/pdf -> pypdf_converter.sources (List[Path])\n",
       "  - file_type_router.text/markdown -> markdown_converter.sources (List[Path])\n",
       "  - text_file_converter.documents -> document_joiner.documents (List[Document])\n",
       "  - markdown_converter.documents -> document_joiner.documents (List[Document])\n",
       "  - pypdf_converter.documents -> document_joiner.documents (List[Document])\n",
       "  - document_joiner.documents -> document_cleaner.documents (List[Document])\n",
       "  - document_cleaner.documents -> document_splitter.documents (List[Document])\n",
       "  - document_splitter.documents -> document_embedder.documents (List[Document])\n",
       "  - document_embedder.documents -> document_writer.documents (List[Document])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = Pipeline()\n",
    "preprocessing_pipeline.add_component(instance=file_type_router, name=\"file_type_router\")\n",
    "preprocessing_pipeline.add_component(instance=text_file_converter, name=\"text_file_converter\")\n",
    "preprocessing_pipeline.add_component(instance=markdown_converter, name=\"markdown_converter\")\n",
    "preprocessing_pipeline.add_component(instance=pdf_converter, name=\"pypdf_converter\")\n",
    "preprocessing_pipeline.add_component(instance=document_joiner, name=\"document_joiner\")\n",
    "preprocessing_pipeline.add_component(instance=document_cleaner, name=\"document_cleaner\")\n",
    "preprocessing_pipeline.add_component(instance=document_splitter, name=\"document_splitter\")\n",
    "preprocessing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
    "preprocessing_pipeline.add_component(instance=document_writer, name=\"document_writer\")\n",
    "\n",
    "preprocessing_pipeline.connect(\"file_type_router.text/plain\", \"text_file_converter.sources\")\n",
    "preprocessing_pipeline.connect(\"file_type_router.application/pdf\", \"pypdf_converter.sources\")\n",
    "preprocessing_pipeline.connect(\"file_type_router.text/markdown\", \"markdown_converter.sources\")\n",
    "preprocessing_pipeline.connect(\"text_file_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"pypdf_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"markdown_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"document_joiner\", \"document_cleaner\")\n",
    "preprocessing_pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
    "preprocessing_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
    "preprocessing_pipeline.connect(\"document_embedder\", \"document_writer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngohongthai/miniconda3/envs/nmt/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Converting markdown files to Documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 635.79it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'document_writer': {'documents_written': 7}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "preprocessing_pipeline.run({\"file_type_router\": {\"sources\": list(Path(output_dir).glob(\"**/*\"))}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional) Build a pipeline to query documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"HF_API_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HF_API_TOKEN\"] = getpass(\"Enter Hugging Face token:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x31fe97b20>\n",
       "ðŸš… Components\n",
       "  - embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: HuggingFaceAPIGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import HuggingFaceAPIGenerator\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the questions based on the given context.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ question }}\n",
    "Answer:\n",
    "\"\"\"\n",
    "pipe = Pipeline()\n",
    "pipe.add_component(\"embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
    "pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
    "pipe.add_component(\n",
    "    \"llm\",\n",
    "    HuggingFaceAPIGenerator(api_type=\"serverless_inference_api\", api_params={\"model\": \"HuggingFaceH4/zephyr-7b-beta\"}),\n",
    ")\n",
    "\n",
    "pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
    "pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "pipe.connect(\"prompt_builder\", \"llm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.83s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'replies': [\"\\n\\nVegan Keto Eggplant Lasagna:\\n\\nIngredients:\\n- 2 large eggplants\\n- A lot of salt (you should have this in your house already)\\n- 1/2 cup store-bought vegan mozzarella (for topping)\\n\\nPesto:\\n- 4 oz basil (generally one large clamshell or 2 small ones)\\n- 1/4 cup almonds\\n- 1/4 cup nutritional yeast\\n- 1/4 cup olive oil\\n- 1 recipe vegan pesto (you can find this in the recipe)\\n- 1 recipe spinach tofu ricotta (you can find this in the recipe)\\n- 1 tsp garlic powder\\n- Juice of half a lemon\\n- Salt to taste\\n\\nSpinach Tofu Ricotta:\\n- 10 oz firm or extra firm tofu\\n- Juice of 1 lemon\\n- Garlic powder to taste\\n- Salt to taste\\n\\nInstructions:\\n1. Slice the eggplants into 1/4 inch thick slices. Some slices will need to be scrapped because it's difficult to get them all uniformly thin. Use them in soup or something, IDK, man.\\n2. Take the eggplant slices and rub both sides with salt. Don't be shy about how much, you're gonna rinse it off anyway.\\n3. Put them in a colander with something underneath it and let them sit for half an hour. This draws the water out so that the egg\"],\n",
       "  'meta': [{'model': 'HuggingFaceH4/zephyr-7b-beta',\n",
       "    'finish_reason': None,\n",
       "    'usage': {'completion_tokens': 0}}]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = (\n",
    "    \"What ingredients would I need to make vegan keto eggplant lasagna, vegan persimmon flan, and vegan hemp cheese?\"\n",
    ")\n",
    "\n",
    "pipe.run(\n",
    "    {\n",
    "        \"embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "        \"llm\": {\"generation_kwargs\": {\"max_new_tokens\": 350}},\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
